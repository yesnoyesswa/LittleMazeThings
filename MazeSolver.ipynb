{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fcd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mazegenfromc import generate_maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527440d",
   "metadata": {},
   "source": [
    "Tạo mê cung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20386818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mazetensor(maze_np):\n",
    "    maze_tensor = torch.from_numpy(maze_np).float()\n",
    "    \n",
    "    # Batch_size, Channels, Height, Width\n",
    "    # Số lượng, số lớp, chiều cao, chiều rộng\n",
    "    #  1 ảnh, 1 màu, cao 10, rộng 10\n",
    "    maze_tensor = maze_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return maze_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b9ddb",
   "metadata": {},
   "source": [
    "Hàm lấy vị trí đích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27aa2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_position(maze):\n",
    "\n",
    "    result = np.where(maze == 9)\n",
    "    \n",
    "    y = int(result[0][0])\n",
    "    x = int(result[1][0])\n",
    "    return (y, x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4273e",
   "metadata": {},
   "source": [
    "Hàm lấy vị trí agent hiện tại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d1a0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_position(maze):\n",
    "    \n",
    "    result = np.where(maze == 2)\n",
    "    \n",
    "    y = int(result[0][0])\n",
    "    x = int(result[1][0])\n",
    "    return (y, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a0778",
   "metadata": {},
   "source": [
    "Hàm lấy tầm nhìn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc81fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_9x9_view(maze_np, heatmap_np, agent_position):\n",
    "    pad_size = 4\n",
    "    \n",
    "    # Padding\n",
    "    padded_maze = np.pad(maze_np, pad_size, mode='constant', constant_values=1)\n",
    "    padded_heat = np.pad(heatmap_np, pad_size, mode='constant', constant_values=99)\n",
    "    \n",
    "    y, x = agent_position[0] + pad_size, agent_position[1] + pad_size\n",
    "    \n",
    "    # Cắt vùng 9x9\n",
    "    maze_cut = padded_maze[y-4:y+5, x-4:x+5]\n",
    "    heat_cut = padded_heat[y-4:y+5, x-4:x+5]\n",
    "    \n",
    "    heat_norm = 1.0 / (1.0 + heat_cut)\n",
    "\n",
    "    heat_norm[maze_cut == 1] = 0.0\n",
    "    \n",
    "    # Chồng thành Tensor [2, 9, 9]\n",
    "    stack_map = torch.stack([\n",
    "        torch.from_numpy(maze_cut).float(),\n",
    "        torch.from_numpy(heat_norm).float()\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Thêm chiều [1, 2, 9, 9]\n",
    "    return stack_map.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93e7d0",
   "metadata": {},
   "source": [
    "Hàm lấy vector chỉ hướng đích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c19ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_vector(agent_position, goal_position, maze_size):\n",
    "\n",
    "    #agent_pos: (y, x)\n",
    "    #goal_pos: (y, x)\n",
    "    #maze_size: (H, W)\n",
    "\n",
    "    H, W = maze_size\n",
    "    \n",
    "    # 1. Tính khoảng cách thô\n",
    "    dy = goal_position[0] - agent_position[0]\n",
    "    dx = goal_position[1] - agent_position[1]\n",
    "    \n",
    "    # 2. Chuẩn hóa về khoảng [-1, 1]\n",
    "    dy_norm = dy / H\n",
    "    dx_norm = dx / W\n",
    "    \n",
    "    return torch.tensor([dy_norm, dx_norm], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2cc6c",
   "metadata": {},
   "source": [
    "Hàm cập nhật ô đã đi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c21b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_visit_count(heatmap, agent_position):\n",
    "    y, x = agent_position\n",
    "    heatmap[y, x] += 1\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923934c1",
   "metadata": {},
   "source": [
    "Tạo hàm ánh xạ số trong lớp quyết định và hướng đi của agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7baf0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(current_position, action):\n",
    "\n",
    "    y, x = current_position\n",
    "    \n",
    "    action_map = {\n",
    "        0: (-1, 0), # Lên\n",
    "        1: (1, 0),  # Xuống\n",
    "        2: (0, -1), # Trái\n",
    "        3: (0, 1)   # Phải\n",
    "    }\n",
    "    \n",
    "    dy, dx = action_map[action]\n",
    "    next_y = y + dy\n",
    "    next_x = x + dx\n",
    "    \n",
    "    return (next_y, next_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc96a2e",
   "metadata": {},
   "source": [
    "Tạo các lớp tích chập (convolution layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1db4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = nn.Sequential(\n",
    "    # Lớp 1: 9*9 -> 7*7\n",
    "    nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3), \n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 2: 7*7 -> 5*5\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 3: 5*5 -> 3*3\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 4: 3*3 -> 1*1\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55697fe",
   "metadata": {},
   "source": [
    "Tạo lớp quyết định"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e1f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_layer = nn.Linear(66, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a87bfd",
   "metadata": {},
   "source": [
    "Tạo hàm dự đoán bước tiếp theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaceff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_move(maze_np, heatmap_np, conv_layers, decision_layer, goal_position):\n",
    "    with torch.no_grad():\n",
    "        current_position = get_current_position(maze_np)\n",
    "\n",
    "        live_view = get_9x9_view(maze_np, heatmap_np, current_position)\n",
    "        view_4d = conv_layers(live_view)\n",
    "        view_flat = torch.flatten(view_4d, start_dim=1)  # [1, 64, 1, 1] -> [1, 64]\n",
    "    \n",
    "        goal_vec = get_goal_vector(current_position, goal_position, maze_np.shape).unsqueeze(0)  # [dy_norm, dx_norm] -> [[dy_norm, dx_norm]]\n",
    "    \n",
    "        final_input = torch.cat((view_flat, goal_vec), dim=1)\n",
    "    \n",
    "        action_logits = decision_layer(final_input)\n",
    "        action = torch.argmax(action_logits, dim=1).item()\n",
    "    \n",
    "        next_position = get_next_position(current_position, action)\n",
    "    \n",
    "    return action, next_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d47e2d",
   "metadata": {},
   "source": [
    "Tạo hàm bước đi và hệ quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce13dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(action, maze_np, heatmap_np, goal_position):\n",
    "\n",
    "    curr_position = get_current_position(maze_np)\n",
    "    old_dist = abs(goal_position[0] - curr_position[0]) + abs(goal_position[1] - curr_position[1])\n",
    "    \n",
    "    next_position = get_next_position(curr_position, action)\n",
    "    nexty, nextx = next_position\n",
    "    \n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    if nexty < 0 or nexty >= maze_np.shape[0] or nextx < 0 or nextx >= maze_np.shape[1] or maze_np[nexty, nextx] == 1:\n",
    "        return -0.8, False \n",
    "        \n",
    "    if maze_np[nexty, nextx] == 9:\n",
    "        return 10.0, True\n",
    "        \n",
    "    # Lại gần đích thì thưởng, xa đích thì phạt\n",
    "    new_dist = abs(goal_position[0] - nexty) + abs(goal_position[1] - nextx)\n",
    "    reward = (old_dist - new_dist) * 0.2 \n",
    "    \n",
    "    if heatmap_np[nexty, nextx] > 0:\n",
    "        reward -= 0.1 * heatmap_np[nexty, nextx] \n",
    "    else:\n",
    "        reward -= 0.01 \n",
    "    \n",
    "    # Cập nhật trạng thái 2 lớp mê cung\n",
    "    maze_np[curr_position[0], curr_position[1]] = 0\n",
    "    maze_np[nexty, nextx] = 2\n",
    "    update_visit_count(heatmap_np, next_position)\n",
    "    \n",
    "    return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc757ef",
   "metadata": {},
   "source": [
    "Chọn optimizer và cách tính loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "484889e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parameters = list(conv_layers.parameters()) + list(decision_layer.parameters())\n",
    "optimizer = optim.Adam(all_parameters, lr=0.001)\n",
    "\n",
    "losscal = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e18b25",
   "metadata": {},
   "source": [
    "Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f48ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_state(maze_np, heatmap_np, goal_position):\n",
    "\n",
    "    curr_position = get_current_position(maze_np)\n",
    "    view = get_9x9_view(maze_np, heatmap_np, curr_position)\n",
    "    \n",
    "    goal_vec = get_goal_vector(curr_position, goal_position, maze_np.shape).unsqueeze(0) # [2] -> [1, 2]\n",
    "    \n",
    "    return (view, goal_vec)\n",
    "\n",
    "def train_one_step(state, action, reward, done, goal_position, maze_np, heatmap_np, gamma=0.95):\n",
    "\n",
    "    # Q-predict\n",
    "    view, goal_vec = state\n",
    "\n",
    "    view_out = conv_layers(view)\n",
    "    view_flat = torch.flatten(view_out, start_dim = 1) # [1, 64, 1, 1] -> [1, 64]\n",
    "    combined = torch.cat((view_flat, goal_vec), dim=1) # [1, 64] + [1, 2] -> [1, 66]\n",
    "    \n",
    "    current_logits = decision_layer(combined)\n",
    "    q_value = current_logits[0, action] \n",
    "\n",
    "    # Q-target\n",
    "    next_state = capture_state(maze_np, heatmap_np, goal_position)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if done:\n",
    "            q_target = torch.tensor(reward, dtype=torch.float32)\n",
    "        else:\n",
    "            next_view, next_goal_vec = next_state \n",
    "            \n",
    "            nv_out = conv_layers(next_view)\n",
    "            nv_flat = torch.flatten(nv_out, 1)\n",
    "            n_combined = torch.cat((nv_flat, next_goal_vec), dim=1) \n",
    "            \n",
    "            next_logits = decision_layer(n_combined)\n",
    "            \n",
    "            q_target = reward + gamma * torch.max(next_logits) # Bellman equation\n",
    "\n",
    "    # Hạ dốc sai số (Gradient Descent)\n",
    "    loss = losscal(q_value, q_target)\n",
    "    \n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()      \n",
    "    optimizer.step()     \n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152a32b",
   "metadata": {},
   "source": [
    "Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bd3fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_buffer = []\n",
    "limit_memory = 10000\n",
    "\n",
    "def store_memory(state, action, reward, next_state, done):\n",
    "    if len(memory_buffer) >= limit_memory:\n",
    "        memory_buffer.pop(0)\n",
    "    memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "def train_from_memory(batch_size=32, gamma=0.95):\n",
    "    if len(memory_buffer) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    batch = random.sample(memory_buffer, batch_size)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for state, action, reward, next_state, done in batch:\n",
    "    \n",
    "        view, goal_vec = state\n",
    "        v_out = conv_layers(view) \n",
    "        v_flat = torch.flatten(v_out, 1)\n",
    "        combined = torch.cat((v_flat, goal_vec), dim=1)\n",
    "        q_values = decision_layer(combined) \n",
    "        q_value = q_values[0, action]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if done:\n",
    "                q_target = torch.tensor(reward, dtype=torch.float32)\n",
    "            else:\n",
    "                nv, ng = next_state\n",
    "                nv_out = conv_layers(nv)\n",
    "                nv_flat = torch.flatten(nv_out, 1)\n",
    "                n_combined = torch.cat((nv_flat, ng), dim=1)\n",
    "                next_q = decision_layer(n_combined)\n",
    "                q_target = reward + gamma * torch.max(next_q)\n",
    "        \n",
    "        loss = losscal(q_value, q_target)\n",
    "        total_loss += loss\n",
    "        \n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad() \n",
    "    avg_loss = total_loss / batch_size\n",
    "    avg_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return avg_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e0488",
   "metadata": {},
   "source": [
    "Đóng gói các hàm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf5d5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_store_and_learn(maze_np, heatmap_np, goal_position, random_rate):\n",
    "\n",
    "    state_before = capture_state(maze_np, heatmap_np, goal_position)\n",
    "    \n",
    "    # Cơ chế Epsilon-Greedy \n",
    "    if np.random.rand() < random_rate:\n",
    "        action = np.random.randint(0, 4) # Chọn ngẫu nhiên 1 trong 4 hướng\n",
    "    else:\n",
    "        action, _ = predict_move(maze_np, heatmap_np, conv_layers, decision_layer, goal_position)\n",
    "    \n",
    "    reward, done = move(action, maze_np, heatmap_np, goal_position)\n",
    "\n",
    "    state_after = capture_state(maze_np, heatmap_np, goal_position)\n",
    "    \n",
    "    #Lưu\n",
    "    store_memory(state_before, action, reward, state_after, done)\n",
    "    \n",
    "    # Train\n",
    "    loss = train_one_step(state_before, action, reward, done, goal_position, maze_np, heatmap_np)\n",
    "    \n",
    "    return reward, done, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename=\"maze_ai_checkpoint.pth\", ep=0, epsilon=0.1):\n",
    "    checkpoint = {\n",
    "        'conv_state': conv_layers.state_dict(),       # Lưu lớp CNN\n",
    "        'decision_state': decision_layer.state_dict(), # Lưu lớp Quyết định\n",
    "        'optimizer_state': optimizer.state_dict(),     # Lưu trạng thái Optimizer                                \n",
    "        'epsilon': epsilon                             \n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Đã lưu tham số vào file {filename}\")\n",
    "\n",
    "def load_checkpoint(filename=\"maze_ai_checkpoint.pth\"):\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load(filename, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Nạp trọng số vào các lớp\n",
    "    conv_layers.load_state_dict(checkpoint['conv_state'])\n",
    "    decision_layer.load_state_dict(checkpoint['decision_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    \n",
    "    epsilon = checkpoint['epsilon']\n",
    "    \n",
    "    print(f\"Đã tải thành công! Tiếp tục ...\")\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0f3d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu quá trình huấn luyện AI...\n",
      "Ván   0 | THẤT BẠI | Bước: 1000 | Tổng điểm: -507.69 | Loss: 0.0015\n",
      "Ván   1 | THÀNH CÔNG | Bước: 478 | Tổng điểm: -49.43 | Loss: 35.7773\n",
      "Ván   2 | THẤT BẠI | Bước: 1000 | Tổng điểm: -465.38 | Loss: 0.0157\n",
      "Ván   3 | THẤT BẠI | Bước: 1000 | Tổng điểm: -206.29 | Loss: 0.0758\n",
      "Ván   4 | THẤT BẠI | Bước: 1000 | Tổng điểm: -229.10 | Loss: 0.1885\n",
      "Ván   5 | THẤT BẠI | Bước: 1000 | Tổng điểm: -205.60 | Loss: 0.0203\n",
      "Ván   6 | THẤT BẠI | Bước: 1000 | Tổng điểm: -427.74 | Loss: 0.0153\n",
      "Ván   7 | THÀNH CÔNG | Bước: 556 | Tổng điểm: -91.77 | Loss: 335.1788\n",
      "Ván   8 | THẤT BẠI | Bước: 1000 | Tổng điểm: -225.89 | Loss: 0.0004\n",
      "Ván   9 | THẤT BẠI | Bước: 1000 | Tổng điểm: -231.51 | Loss: 0.1100\n",
      "Ván  10 | THẤT BẠI | Bước: 1000 | Tổng điểm: -233.86 | Loss: 1.0045\n",
      "Ván  11 | THẤT BẠI | Bước: 1000 | Tổng điểm: -191.02 | Loss: 0.1419\n",
      "Ván  12 | THÀNH CÔNG | Bước: 623 | Tổng điểm: -75.29 | Loss: 32.1550\n",
      "Ván  13 | THẤT BẠI | Bước: 1000 | Tổng điểm: -431.05 | Loss: 8.1963\n",
      "Ván  14 | THẤT BẠI | Bước: 1000 | Tổng điểm: -1012.84 | Loss: 0.6441\n",
      "Ván  15 | THẤT BẠI | Bước: 1000 | Tổng điểm: -680.48 | Loss: 1.1429\n",
      "Ván  16 | THẤT BẠI | Bước: 1000 | Tổng điểm: -1031.23 | Loss: 1.5590\n",
      "Ván  17 | THẤT BẠI | Bước: 1000 | Tổng điểm: -610.48 | Loss: 6.7133\n",
      "Ván  18 | THẤT BẠI | Bước: 1000 | Tổng điểm: -861.00 | Loss: 0.7560\n",
      "Ván  19 | THẤT BẠI | Bước: 1000 | Tổng điểm: -772.76 | Loss: 0.0722\n",
      "Ván  20 | THẤT BẠI | Bước: 1000 | Tổng điểm: -877.23 | Loss: 0.5255\n",
      "Ván  21 | THẤT BẠI | Bước: 1000 | Tổng điểm: -650.47 | Loss: 0.0855\n",
      "Ván  22 | THẤT BẠI | Bước: 1000 | Tổng điểm: -724.43 | Loss: 0.0001\n",
      "Ván  23 | THẤT BẠI | Bước: 1000 | Tổng điểm: -534.24 | Loss: 1.3798\n",
      "Ván  24 | THẤT BẠI | Bước: 1000 | Tổng điểm: -756.83 | Loss: 0.0049\n",
      "Ván  25 | THẤT BẠI | Bước: 1000 | Tổng điểm: -637.40 | Loss: 0.0768\n",
      "Ván  26 | THẤT BẠI | Bước: 1000 | Tổng điểm: -543.98 | Loss: 1.6114\n",
      "Ván  27 | THẤT BẠI | Bước: 1000 | Tổng điểm: -599.94 | Loss: 0.0619\n",
      "Ván  28 | THẤT BẠI | Bước: 1000 | Tổng điểm: -492.28 | Loss: 0.2764\n",
      "Ván  29 | THẤT BẠI | Bước: 1000 | Tổng điểm: -303.64 | Loss: 0.2028\n",
      "Ván  30 | THẤT BẠI | Bước: 1000 | Tổng điểm: -333.96 | Loss: 0.0351\n",
      "Ván  31 | THẤT BẠI | Bước: 1000 | Tổng điểm: -313.87 | Loss: 0.0021\n",
      "Ván  32 | THẤT BẠI | Bước: 1000 | Tổng điểm: -547.33 | Loss: 0.0004\n",
      "Ván  33 | THẤT BẠI | Bước: 1000 | Tổng điểm: -613.45 | Loss: 0.0072\n",
      "Ván  34 | THÀNH CÔNG | Bước: 761 | Tổng điểm: -208.80 | Loss: 11.4837\n",
      "Ván  35 | THẤT BẠI | Bước: 1000 | Tổng điểm: -497.82 | Loss: 0.0001\n",
      "Ván  36 | THẤT BẠI | Bước: 1000 | Tổng điểm: -181.52 | Loss: 0.3751\n",
      "Ván  37 | THẤT BẠI | Bước: 1000 | Tổng điểm: -237.95 | Loss: 0.0016\n",
      "Ván  38 | THẤT BẠI | Bước: 1000 | Tổng điểm: -208.56 | Loss: 0.2183\n",
      "Ván  39 | THẤT BẠI | Bước: 1000 | Tổng điểm: -284.12 | Loss: 0.9419\n",
      "Ván  40 | THẤT BẠI | Bước: 1000 | Tổng điểm: -165.79 | Loss: 0.0163\n",
      "Ván  41 | THÀNH CÔNG | Bước: 393 | Tổng điểm: -12.88 | Loss: 68.3359\n",
      "Ván  42 | THẤT BẠI | Bước: 1000 | Tổng điểm: -443.56 | Loss: 1.8244\n",
      "Ván  43 | THẤT BẠI | Bước: 1000 | Tổng điểm: -313.23 | Loss: 0.0432\n",
      "Ván  44 | THẤT BẠI | Bước: 1000 | Tổng điểm: -612.51 | Loss: 0.0000\n",
      "Ván  45 | THẤT BẠI | Bước: 1000 | Tổng điểm: -188.87 | Loss: 0.0082\n",
      "Ván  46 | THẤT BẠI | Bước: 1000 | Tổng điểm: -497.93 | Loss: 0.0003\n",
      "Ván  47 | THẤT BẠI | Bước: 1000 | Tổng điểm: -400.03 | Loss: 0.0791\n",
      "Ván  48 | THẤT BẠI | Bước: 1000 | Tổng điểm: -300.24 | Loss: 0.0394\n",
      "Ván  49 | THÀNH CÔNG | Bước: 388 | Tổng điểm:  -4.17 | Loss: 11.3914\n",
      "Ván  50 | THẤT BẠI | Bước: 1000 | Tổng điểm: -317.90 | Loss: 0.0026\n",
      "Ván  51 | THÀNH CÔNG | Bước: 506 | Tổng điểm: -47.32 | Loss: 23.1949\n",
      "Ván  52 | THẤT BẠI | Bước: 1000 | Tổng điểm: -647.91 | Loss: 0.0012\n",
      "Ván  53 | THÀNH CÔNG | Bước: 448 | Tổng điểm: -28.34 | Loss: 114.2718\n",
      "Ván  54 | THẤT BẠI | Bước: 1000 | Tổng điểm: -272.80 | Loss: 0.1445\n",
      "Ván  55 | THẤT BẠI | Bước: 1000 | Tổng điểm: -282.66 | Loss: 0.0116\n",
      "Ván  56 | THẤT BẠI | Bước: 1000 | Tổng điểm: -261.56 | Loss: 0.3242\n",
      "Ván  57 | THẤT BẠI | Bước: 1000 | Tổng điểm: -171.03 | Loss: 0.0004\n",
      "Ván  58 | THẤT BẠI | Bước: 1000 | Tổng điểm: -353.28 | Loss: 0.0768\n",
      "Ván  59 | THẤT BẠI | Bước: 1000 | Tổng điểm: -433.02 | Loss: 0.0028\n",
      "Ván  60 | THẤT BẠI | Bước: 1000 | Tổng điểm: -238.66 | Loss: 0.0382\n",
      "Ván  61 | THÀNH CÔNG | Bước: 840 | Tổng điểm: -135.39 | Loss: 179.8861\n",
      "Ván  62 | THÀNH CÔNG | Bước: 318 | Tổng điểm:   8.04 | Loss: 44.2654\n",
      "Ván  63 | THÀNH CÔNG | Bước: 216 | Tổng điểm:  22.61 | Loss: 4.3695\n",
      "Ván  64 | THẤT BẠI | Bước: 1000 | Tổng điểm: -246.66 | Loss: 0.0117\n",
      "Ván  65 | THẤT BẠI | Bước: 1000 | Tổng điểm: -178.76 | Loss: 0.0269\n",
      "Ván  66 | THẤT BẠI | Bước: 1000 | Tổng điểm: -207.72 | Loss: 0.2204\n",
      "Ván  67 | THẤT BẠI | Bước: 1000 | Tổng điểm: -242.75 | Loss: 0.0104\n",
      "Ván  68 | THẤT BẠI | Bước: 1000 | Tổng điểm: -504.44 | Loss: 0.5919\n",
      "Ván  69 | THẤT BẠI | Bước: 1000 | Tổng điểm: -211.35 | Loss: 0.2006\n",
      "Ván  70 | THÀNH CÔNG | Bước: 669 | Tổng điểm: -94.27 | Loss: 97.0380\n",
      "Ván  71 | THẤT BẠI | Bước: 1000 | Tổng điểm: -325.15 | Loss: 0.0003\n",
      "Ván  72 | THẤT BẠI | Bước: 1000 | Tổng điểm: -302.84 | Loss: 0.2206\n",
      "Ván  73 | THẤT BẠI | Bước: 1000 | Tổng điểm: -281.04 | Loss: 0.0151\n",
      "Ván  74 | THẤT BẠI | Bước: 1000 | Tổng điểm: -262.09 | Loss: 0.0005\n",
      "Ván  75 | THÀNH CÔNG | Bước: 895 | Tổng điểm: -132.05 | Loss: 77.2481\n",
      "Ván  76 | THẤT BẠI | Bước: 1000 | Tổng điểm: -351.27 | Loss: 0.0206\n",
      "Ván  77 | THÀNH CÔNG | Bước: 514 | Tổng điểm: -44.53 | Loss: 131.1772\n",
      "Ván  78 | THẤT BẠI | Bước: 1000 | Tổng điểm: -483.90 | Loss: 0.0001\n",
      "Ván  79 | THẤT BẠI | Bước: 1000 | Tổng điểm: -377.01 | Loss: 0.0000\n",
      "Ván  80 | THẤT BẠI | Bước: 1000 | Tổng điểm: -686.73 | Loss: 0.0279\n",
      "Ván  81 | THẤT BẠI | Bước: 1000 | Tổng điểm: -208.80 | Loss: 0.2787\n",
      "Ván  82 | THẤT BẠI | Bước: 1000 | Tổng điểm: -138.74 | Loss: 0.0002\n",
      "Ván  83 | THÀNH CÔNG | Bước: 916 | Tổng điểm: -109.07 | Loss: 4.8217\n",
      "Ván  84 | THÀNH CÔNG | Bước: 700 | Tổng điểm: -85.14 | Loss: 27.7335\n",
      "Ván  85 | THẤT BẠI | Bước: 1000 | Tổng điểm: -321.89 | Loss: 0.0181\n",
      "Ván  86 | THẤT BẠI | Bước: 1000 | Tổng điểm: -535.06 | Loss: 0.2980\n",
      "Ván  87 | THẤT BẠI | Bước: 1000 | Tổng điểm: -661.46 | Loss: 0.5048\n",
      "Ván  88 | THẤT BẠI | Bước: 1000 | Tổng điểm: -640.52 | Loss: 0.0015\n",
      "Ván  89 | THẤT BẠI | Bước: 1000 | Tổng điểm: -487.20 | Loss: 1.4126\n",
      "Ván  90 | THẤT BẠI | Bước: 1000 | Tổng điểm: -740.52 | Loss: 0.7321\n",
      "Ván  91 | THẤT BẠI | Bước: 1000 | Tổng điểm: -613.71 | Loss: 27145.2969\n",
      "Ván  92 | THẤT BẠI | Bước: 1000 | Tổng điểm: -843.90 | Loss: 0.4427\n",
      "Ván  93 | THẤT BẠI | Bước: 1000 | Tổng điểm: -1052.31 | Loss: 1.0736\n",
      "Ván  94 | THẤT BẠI | Bước: 1000 | Tổng điểm: -772.13 | Loss: 0.0647\n",
      "Ván  95 | THẤT BẠI | Bước: 1000 | Tổng điểm: -772.50 | Loss: 0.3728\n",
      "Ván  96 | THẤT BẠI | Bước: 1000 | Tổng điểm: -830.08 | Loss: 0.3131\n",
      "Ván  97 | THẤT BẠI | Bước: 1000 | Tổng điểm: -664.08 | Loss: 0.3061\n",
      "Ván  98 | THẤT BẠI | Bước: 1000 | Tổng điểm: -737.50 | Loss: 0.1557\n",
      "Ván  99 | THẤT BẠI | Bước: 1000 | Tổng điểm: -781.22 | Loss: 0.0002\n",
      "Ván 100 | THẤT BẠI | Bước: 1000 | Tổng điểm: -699.01 | Loss: 0.0436\n",
      "Ván 101 | THẤT BẠI | Bước: 1000 | Tổng điểm: -726.05 | Loss: 0.0241\n",
      "Ván 102 | THẤT BẠI | Bước: 1000 | Tổng điểm: -770.84 | Loss: 0.0086\n",
      "Ván 103 | THẤT BẠI | Bước: 1000 | Tổng điểm: -740.19 | Loss: 0.0064\n",
      "Ván 104 | THẤT BẠI | Bước: 1000 | Tổng điểm: -736.73 | Loss: 0.0306\n",
      "Ván 105 | THẤT BẠI | Bước: 1000 | Tổng điểm: -756.79 | Loss: 0.0076\n",
      "Ván 106 | THẤT BẠI | Bước: 1000 | Tổng điểm: -766.20 | Loss: 0.0022\n",
      "Ván 107 | THẤT BẠI | Bước: 1000 | Tổng điểm: -813.12 | Loss: 0.0000\n",
      "Ván 108 | THẤT BẠI | Bước: 1000 | Tổng điểm: -763.48 | Loss: 0.0007\n",
      "Ván 109 | THẤT BẠI | Bước: 1000 | Tổng điểm: -761.21 | Loss: 0.0040\n",
      "Ván 110 | THẤT BẠI | Bước: 1000 | Tổng điểm: -792.41 | Loss: 0.0005\n",
      "Ván 111 | THẤT BẠI | Bước: 1000 | Tổng điểm: -746.16 | Loss: 0.1929\n",
      "Ván 112 | THẤT BẠI | Bước: 1000 | Tổng điểm: -792.68 | Loss: 0.0004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m total_reward = \u001b[32m0\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m step_count < max_steps_per_episode:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     reward, done, loss = \u001b[43mmove_store_and_learn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaze_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheatmap_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     batch_loss = train_from_memory(batch_size=\u001b[32m32\u001b[39m)\n\u001b[32m     23\u001b[39m     total_reward += reward\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mmove_store_and_learn\u001b[39m\u001b[34m(maze_np, heatmap_np, goal_position, random_rate)\u001b[39m\n\u001b[32m     16\u001b[39m store_memory(state_before, action, reward, state_after, done)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m loss = \u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_before\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaze_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheatmap_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reward, done, loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_one_step\u001b[39m\u001b[34m(state, action, reward, done, goal_position, maze_np, heatmap_np, gamma)\u001b[39m\n\u001b[32m     40\u001b[39m loss = losscal(q_value, q_target)\n\u001b[32m     42\u001b[39m optimizer.zero_grad() \n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m      \n\u001b[32m     44\u001b[39m optimizer.step()     \n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HeLi\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HeLi\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HeLi\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 1000  # Tổng số ván chơi để huấn luyện\n",
    "max_steps_per_episode = 1000  # Giới hạn số bước mỗi ván để tránh đi luẩn quẩn\n",
    "random_rate = 0.1  # Tỷ lệ 20% đi ngẫu nhiên để khám phá mê cung\n",
    "\n",
    "print(\"Bắt đầu quá trình huấn luyện AI...\")\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    # 1. Khởi tạo môi trường mới cho mỗi ván\n",
    "    maze_np = generate_maze(30) \n",
    "    heatmap_np = np.zeros_like(maze_np, dtype=np.float32)\n",
    "    goal_position = get_goal_position(maze_np)\n",
    "    \n",
    "    done = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done and step_count < max_steps_per_episode:\n",
    "\n",
    "        reward, done, loss = move_store_and_learn(maze_np, heatmap_np, goal_position, random_rate=random_rate)\n",
    "        \n",
    "        batch_loss = train_from_memory(batch_size=32)\n",
    "        \n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "        \n",
    "    # In kết quả sau mỗi ván để theo dõi tiến độ\n",
    "    status = \"THÀNH CÔNG\" if done else \"THẤT BẠI\"\n",
    "    print(f\"Ván {ep:3d} | {status} | Bước: {step_count:3d} | Tổng điểm: {total_reward:6.2f} | Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "    if random_rate > 0.01:\n",
    "        random_rate *= 0.995"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
