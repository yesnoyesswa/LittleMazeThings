{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fcd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from mazegenfromc import generate_maze\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527440d",
   "metadata": {},
   "source": [
    "Tạo mê cung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20386818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mazetensor(maze_np):\n",
    "    maze_tensor = torch.from_numpy(maze_np).float()\n",
    "    \n",
    "    # Batch_size, Channels, Height, Width\n",
    "    # Số lượng, số lớp, chiều cao, chiều rộng\n",
    "    #  1 ảnh, 1 màu, cao 10, rộng 10\n",
    "    maze_tensor = maze_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return maze_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b9ddb",
   "metadata": {},
   "source": [
    "Hàm lấy vị trí đích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27aa2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_position(maze):\n",
    "\n",
    "    result = np.where(maze == 9)\n",
    "    \n",
    "    y = int(result[0][0])\n",
    "    x = int(result[1][0])\n",
    "    return (y, x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4273e",
   "metadata": {},
   "source": [
    "Hàm lấy vị trí agent hiện tại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1a0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_position(maze):\n",
    "    \n",
    "    result = np.where(maze == 2)\n",
    "    \n",
    "    y = int(result[0][0])\n",
    "    x = int(result[1][0])\n",
    "    return (y, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a0778",
   "metadata": {},
   "source": [
    "Hàm lấy tầm nhìn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc81fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_9x9_view(maze_np, heatmap_np, agent_position):\n",
    "    pad_size = 4\n",
    "    \n",
    "    # Padding\n",
    "    padded_maze = np.pad(maze_np, pad_size, mode='constant', constant_values=1)\n",
    "    padded_heat = np.pad(heatmap_np, pad_size, mode='constant', constant_values=99)\n",
    "    \n",
    "    y, x = agent_position[0] + pad_size, agent_position[1] + pad_size\n",
    "    \n",
    "    # Cắt vùng 9x9\n",
    "    maze_cut = padded_maze[y-4:y+5, x-4:x+5]\n",
    "    heat_cut = padded_heat[y-4:y+5, x-4:x+5]\n",
    "    \n",
    "    heat_norm = 1.0 / (1.0 + heat_cut)\n",
    "\n",
    "    heat_norm[maze_cut == 1] = 0.0\n",
    "    \n",
    "    # Chồng thành Tensor [2, 9, 9]\n",
    "    stack_map = torch.stack([\n",
    "        torch.from_numpy(maze_cut).float(),\n",
    "        torch.from_numpy(heat_norm).float()\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Thêm chiều [1, 2, 9, 9]\n",
    "    return stack_map.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93e7d0",
   "metadata": {},
   "source": [
    "Hàm lấy vector chỉ hướng đích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c19ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_vector(agent_position, goal_position, maze_size):\n",
    "\n",
    "    #agent_pos: (y, x)\n",
    "    #goal_pos: (y, x)\n",
    "    #maze_size: (H, W)\n",
    "\n",
    "    H, W = maze_size\n",
    "    \n",
    "    # 1. Tính khoảng cách thô\n",
    "    dy = goal_position[0] - agent_position[0]\n",
    "    dx = goal_position[1] - agent_position[1]\n",
    "    \n",
    "    # 2. Chuẩn hóa về khoảng [-1, 1]\n",
    "    dy_norm = dy / H\n",
    "    dx_norm = dx / W\n",
    "    \n",
    "    return torch.tensor([dy_norm, dx_norm], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2cc6c",
   "metadata": {},
   "source": [
    "Hàm cập nhật ô đã đi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c21b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_visit_count(heatmap, agent_position):\n",
    "    y, x = agent_position\n",
    "    heatmap[y, x] += 1\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923934c1",
   "metadata": {},
   "source": [
    "Tạo hàm ánh xạ số trong lớp quyết định và hướng đi của agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7baf0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(current_position, action):\n",
    "\n",
    "    y, x = current_position\n",
    "    \n",
    "    action_map = {\n",
    "        0: (-1, 0), # Lên\n",
    "        1: (1, 0),  # Xuống\n",
    "        2: (0, -1), # Trái\n",
    "        3: (0, 1)   # Phải\n",
    "    }\n",
    "    \n",
    "    dy, dx = action_map[action]\n",
    "    next_y = y + dy\n",
    "    next_x = x + dx\n",
    "    \n",
    "    return (next_y, next_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc96a2e",
   "metadata": {},
   "source": [
    "Tạo các lớp tích chập (convolution layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1db4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = nn.Sequential(\n",
    "    # Lớp 1: 9*9 -> 7*7\n",
    "    nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3), \n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 2: 7*7 -> 5*5\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 3: 5*5 -> 3*3\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 4: 3*3 -> 1*1\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "conv_layers = conv_layers.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55697fe",
   "metadata": {},
   "source": [
    "Tạo lớp quyết định"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e1f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_layer = nn.Linear(66, 4)\n",
    "decision_layer = decision_layer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a87bfd",
   "metadata": {},
   "source": [
    "Tạo hàm dự đoán bước tiếp theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaceff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_move(maze_np, heatmap_np, conv_layers, decision_layer, goal_position):\n",
    "    with torch.no_grad():\n",
    "        current_position = get_current_position(maze_np)\n",
    "\n",
    "        live_view = get_9x9_view(maze_np, heatmap_np, current_position)\n",
    "        view_4d = conv_layers(live_view)\n",
    "        view_flat = torch.flatten(view_4d, start_dim=1)  # [1, 64, 1, 1] -> [1, 64]\n",
    "    \n",
    "        goal_vec = get_goal_vector(current_position, goal_position, maze_np.shape).unsqueeze(0)  # [dy_norm, dx_norm] -> [[dy_norm, dx_norm]]\n",
    "    \n",
    "        final_input = torch.cat((view_flat, goal_vec), dim=1)\n",
    "    \n",
    "        action_logits = decision_layer(final_input)\n",
    "        action = torch.argmax(action_logits, dim=1).item()\n",
    "    \n",
    "        next_position = get_next_position(current_position, action)\n",
    "    \n",
    "    return action, next_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d47e2d",
   "metadata": {},
   "source": [
    "Tạo hàm bước đi và hệ quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce13dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(action, maze_np, heatmap_np, goal_position):\n",
    "\n",
    "    curr_position = get_current_position(maze_np)\n",
    "    old_dist = abs(goal_position[0] - curr_position[0]) + abs(goal_position[1] - curr_position[1])\n",
    "    \n",
    "    next_position = get_next_position(curr_position, action)\n",
    "    nexty, nextx = next_position\n",
    "    \n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    if nexty < 0 or nexty >= maze_np.shape[0] or nextx < 0 or nextx >= maze_np.shape[1] or maze_np[nexty, nextx] == 1:\n",
    "        return -1, False \n",
    "        \n",
    "    if maze_np[nexty, nextx] == 9:\n",
    "        return 150.0, True\n",
    "    \n",
    "    if heatmap_np[nexty, nextx] > 0:\n",
    "        reward -= 0.02 * heatmap_np[nexty, nextx] \n",
    "    else:\n",
    "        reward -= 0.01 \n",
    "    \n",
    "    # Cập nhật trạng thái 2 lớp mê cung\n",
    "    maze_np[curr_position[0], curr_position[1]] = 0\n",
    "    maze_np[nexty, nextx] = 2\n",
    "    update_visit_count(heatmap_np, next_position)\n",
    "    \n",
    "    return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc757ef",
   "metadata": {},
   "source": [
    "Chọn optimizer và cách tính loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484889e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parameters = list(conv_layers.parameters()) + list(decision_layer.parameters())\n",
    "optimizer = optim.Adam(all_parameters, lr=0.0001)\n",
    "\n",
    "losscal = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c2198",
   "metadata": {},
   "source": [
    "Tạo mạng target độc lập"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "449b64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_conv = copy.deepcopy(conv_layers).to(device)\n",
    "target_decision = copy.deepcopy(decision_layer).to(device)\n",
    "\n",
    "for p in target_conv.parameters(): p.requires_grad = False\n",
    "for p in target_decision.parameters(): p.requires_grad = False\n",
    "\n",
    "def update_target_network():\n",
    "    target_conv.load_state_dict(conv_layers.state_dict())\n",
    "    target_decision.load_state_dict(decision_layer.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e18b25",
   "metadata": {},
   "source": [
    "Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f48ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_state(maze_np, heatmap_np, goal_position):\n",
    "\n",
    "    curr_position = get_current_position(maze_np)\n",
    "    view = get_9x9_view(maze_np, heatmap_np, curr_position)\n",
    "    \n",
    "    goal_vec = get_goal_vector(curr_position, goal_position, maze_np.shape).unsqueeze(0) # [2] -> [1, 2]\n",
    "    \n",
    "    return (view, goal_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bd3fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_buffer = []\n",
    "limit_memory = 100000\n",
    "\n",
    "def store_memory(state, action, reward, next_state, done):\n",
    "    if len(memory_buffer) >= limit_memory:\n",
    "        memory_buffer.pop(0)\n",
    "    memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "'''\n",
    "batch (List)\n",
    " ├── b_0 (Tuple)\n",
    " │    ├── [0] State: ([view_0], [goal_0])\n",
    " │    ├── [1] Action: 3\n",
    " │    ├── [2] Reward: -0.01\n",
    " │    ├── [3] Next State: ([n_view_0], [n_goal_0])\n",
    " │    └── [4] Done: False\n",
    " ├── b_1 (Tuple)\n",
    " │    ├── [0] State: ([view_1], [goal_1])\n",
    " │    ├── ...\n",
    " ├── ...\n",
    " └── b_31 (Tuple)\n",
    "'''\n",
    "\n",
    "def train_from_memory(batch_size=32, gamma=0.95):\n",
    "    if len(memory_buffer) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    batch = random.sample(memory_buffer, batch_size)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    # Gộp 32 mảng 9x9 thành 1 khối [32, 2, 9, 9]\n",
    "    views = torch.cat([b[0][0] for b in batch]).to(device)\n",
    "    goal_vecs = torch.cat([b[0][1] for b in batch]).to(device)\n",
    "    actions = torch.tensor([b[1] for b in batch], dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32).to(device)\n",
    "    \n",
    "    next_views = torch.cat([b[3][0] for b in batch]).to(device)\n",
    "    next_goal_vecs = torch.cat([b[3][1] for b in batch]).to(device)\n",
    "    dones = torch.tensor([b[4] for b in batch], dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Q-VALUE HIỆN TẠI (Main network)\n",
    "    v_out = conv_layers(views)\n",
    "    v_flat = torch.flatten(v_out, 1)\n",
    "    combined = torch.cat((v_flat, goal_vecs), dim=1)\n",
    "    q_values_all = decision_layer(combined) # [32, 4]\n",
    "    \n",
    "    q_value = q_values_all.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Q-TARGET\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        nv_out = target_conv(next_views)\n",
    "        nv_flat = torch.flatten(nv_out, 1)\n",
    "        n_combined = torch.cat((nv_flat, next_goal_vecs), dim=1)\n",
    "        \n",
    "        # Double DQN\n",
    "        # Main net chọn hành động\n",
    "        v_next_out = conv_layers(next_views)\n",
    "        v_next_flat = torch.flatten(v_next_out, 1)\n",
    "        next_combined_main = torch.cat((v_next_flat, next_goal_vecs), dim=1)\n",
    "        next_actions_main = decision_layer(next_combined_main).argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        # Target net đánh giá hành động\n",
    "        q_next_target_all = target_decision(n_combined)\n",
    "        max_next_q = q_next_target_all.gather(1, next_actions_main).squeeze(1)\n",
    "        \n",
    "        # Công thức Bellman: R + gamma * Q_next * (1 - done)\n",
    "        q_target = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "    loss = losscal(q_value, q_target)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(all_parameters, max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e0488",
   "metadata": {},
   "source": [
    "Đóng gói các hàm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf5d5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_and_store(maze_np, heatmap_np, goal_position, random_rate):\n",
    "\n",
    "    state_before = capture_state(maze_np, heatmap_np, goal_position)\n",
    "    \n",
    "    # Cơ chế Epsilon-Greedy \n",
    "    if np.random.rand() < random_rate:\n",
    "        action = np.random.randint(0, 4) # Chọn ngẫu nhiên 1 trong 4 hướng\n",
    "    else:\n",
    "        action, _ = predict_move(maze_np, heatmap_np, conv_layers, decision_layer, goal_position)\n",
    "    \n",
    "    reward, done = move(action, maze_np, heatmap_np, goal_position)\n",
    "\n",
    "    state_after = capture_state(maze_np, heatmap_np, goal_position)\n",
    "    \n",
    "    #Lưu\n",
    "    store_memory(state_before, action, reward, state_after, done)\n",
    "    \n",
    "    return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629c1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename=\"maze_ai_checkpoint.pth\", ep=0, epsilon=0.1):\n",
    "    checkpoint = {\n",
    "        'conv_state': conv_layers.state_dict(),       \n",
    "        'decision_state': decision_layer.state_dict(), \n",
    "        'optimizer_state': optimizer.state_dict(),                                \n",
    "        'epsilon': epsilon                             \n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Đã lưu tham số vào file {filename}\")\n",
    "\n",
    "def load_checkpoint(filename=\"maze_ai_checkpoint.pth\"):\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load(filename, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Nạp trọng số vào các lớp\n",
    "    conv_layers.load_state_dict(checkpoint['conv_state'])\n",
    "    decision_layer.load_state_dict(checkpoint['decision_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    \n",
    "    epsilon = checkpoint['epsilon']\n",
    "    \n",
    "    print(f\"Đã tải thành công! Tiếp tục ...\")\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12ef0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu tham số vào file maze_ai_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58a83d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7b084d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tải thành công! Tiếp tục ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3d3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu huấn luyện từ Size 21x21...\n",
      "Ván    1 | THẤT BẠI | Size: 21x21 | Bước: 1764 | Điểm: -285.29 | Loss: 0.0029 | Epsilon: 0.200\n",
      "Ván    2 | THẤT BẠI | Size: 21x21 | Bước: 1764 | Điểm: -233.60 | Loss: 0.0008 | Epsilon: 0.200\n",
      "Ván    3 | THÀNH CÔNG | Size: 21x21 | Bước: 1131 | Điểm: 13.04 | Loss: 0.0015 | Epsilon: 0.199\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m reward, done \u001b[38;5;241m=\u001b[39m move_and_store(maze_np, heatmap_np, goal_position, random_rate)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_from_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     35\u001b[0m total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[16], line 43\u001b[0m, in \u001b[0;36mtrain_from_memory\u001b[1;34m(batch_size, gamma)\u001b[0m\n\u001b[0;32m     40\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([b[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Q-VALUE HIỆN TẠI (Main network)\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m v_out \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m v_flat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(v_out, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     45\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((v_flat, goal_vecs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start setting\n",
    "current_size = 23\n",
    "max_size = 35\n",
    "num_episodes = 5000     \n",
    "max_steps = int(current_size * current_size * 4) # Max steps theo size\n",
    "random_rate = 0.2\n",
    "target_update_steps = 1000 # Thời gian đóng băng mạng Target \n",
    "total_steps = 0\n",
    "\n",
    "# Winrate tracker\n",
    "win_history = np.zeros(100) # Lưu 100 ván gần nhất\n",
    "win_idx = 0\n",
    "played_enough = False\n",
    "\n",
    "print(f\"Bắt đầu huấn luyện từ Size {current_size}x{current_size}...\")\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    \n",
    "    maze_np = generate_maze(current_size) \n",
    "    heatmap_np = np.zeros_like(maze_np, dtype=np.float32)\n",
    "    goal_position = get_goal_position(maze_np)\n",
    "    \n",
    "    done = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done and step_count < max_steps:\n",
    "        # Sử dụng epsilon-greedy để di chuyển\n",
    "        reward, done = move_and_store(maze_np, heatmap_np, goal_position, random_rate)\n",
    "        \n",
    "        # Train\n",
    "        loss = train_from_memory(batch_size=32)\n",
    "        \n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        \n",
    "        # Cập nhật mạng Target\n",
    "        if total_steps % target_update_steps == 0:\n",
    "            update_target_network()\n",
    "\n",
    "        step_count += 1\n",
    "    \n",
    "    # Cập nhật win tracker\n",
    "    win_history[win_idx] = 1 if done else 0\n",
    "    win_idx = (win_idx + 1) % 100 # Tự reset win_idx\n",
    "    if win_idx == 0: played_enough = True\n",
    "\n",
    "    # Log thông tin ván đấu\n",
    "    status = \"THÀNH CÔNG\" if done else \"THẤT BẠI\"\n",
    "    print(f\"Ván {ep+1:4d} | {status} | Size: {current_size}x{current_size} | Bước: {step_count} | Điểm: {total_reward:.2f} | Loss: {loss:.4f} | Epsilon: {random_rate:.3f}\")\n",
    "\n",
    "    # Kiểm tra điều kiện upsize\n",
    "    if played_enough:\n",
    "        current_win_rate = np.mean(win_history) \n",
    "        \n",
    "        if current_win_rate >= 0.90 and current_size < max_size:\n",
    "            print(f\"Winrate 100 ván gần nhất: {current_win_rate*100:.1f}%\")\n",
    "            \n",
    "            # Lưu\n",
    "            save_checkpoint(filename=f\"maze_ai_size_{current_size}.pth\", epsilon=random_rate)\n",
    "            \n",
    "            # Upsize\n",
    "            current_size += 2\n",
    "            print(f\"Upsize: {current_size}x{current_size}\")\n",
    "            \n",
    "            # Reset setting\n",
    "            random_rate = 0.4\n",
    "            win_history.fill(0)\n",
    "            win_idx = 0\n",
    "            played_enough = False\n",
    "            memory_buffer.clear()\n",
    "            max_steps = int(current_size * current_size * 4) # Cập nhật lại giới hạn bước\n",
    "            print(f\"Đã reset Epsilon và Memory. Max steps mới: {max_steps}\\n\")\n",
    "\n",
    "    if played_enough and current_win_rate < 0.3 and random_rate < 0.2:\n",
    "        print(\"Increase epsilon due to low winrate\")\n",
    "        random_rate = 0.3\n",
    "\n",
    "    # Epsilon decay rate\n",
    "    if random_rate > 0.05:\n",
    "        random_rate *= 0.998 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
