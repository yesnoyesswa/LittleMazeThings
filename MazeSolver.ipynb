{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fcd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import manim\n",
    "\n",
    "from mazegenfromc import generate_maze\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527440d",
   "metadata": {},
   "source": [
    "Tạo mê cung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20386818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mazetensor(maze_np):\n",
    "    maze_tensor = torch.from_numpy(maze_np).float()\n",
    "    \n",
    "    # Batch_size, Channels, Height, Width\n",
    "    # Số lượng, số lớp, chiều cao, chiều rộng\n",
    "    #  1 ảnh, 1 màu, cao 10, rộng 10\n",
    "    maze_tensor = maze_tensor.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    return maze_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b9ddb",
   "metadata": {},
   "source": [
    "Hàm lấy vị trí đích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27aa2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_position(maze):\n",
    "\n",
    "    result = np.where(maze == 9)\n",
    "    \n",
    "    y = int(result[0][0])\n",
    "    x = int(result[1][0])\n",
    "    return (y, x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4273e",
   "metadata": {},
   "source": [
    "Hàm lấy vị trí agent hiện tại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1a0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_position(maze):\n",
    "    \n",
    "    result = np.where(maze == 2)\n",
    "    \n",
    "    y = int(result[0][0])\n",
    "    x = int(result[1][0])\n",
    "    return (y, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a0778",
   "metadata": {},
   "source": [
    "Hàm lấy tầm nhìn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc81fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_9x9_view(maze_np, heatmap_np, agent_position):\n",
    "    pad_size = 4\n",
    "    \n",
    "    # Padding\n",
    "    padded_maze = np.pad(maze_np, pad_size, mode='constant', constant_values=1)\n",
    "    padded_heat = np.pad(heatmap_np, pad_size, mode='constant', constant_values=99)\n",
    "    \n",
    "    y, x = agent_position[0] + pad_size, agent_position[1] + pad_size\n",
    "    \n",
    "    # Cắt vùng 9x9\n",
    "    maze_cut = padded_maze[y-4:y+5, x-4:x+5]\n",
    "    heat_cut = padded_heat[y-4:y+5, x-4:x+5]\n",
    "    \n",
    "    heat_norm = 1.0 / (1.0 + heat_cut)\n",
    "\n",
    "    heat_norm[maze_cut == 1] = 0.0\n",
    "    \n",
    "    # Chồng thành Tensor [2, 9, 9]\n",
    "    stack_map = torch.stack([\n",
    "        torch.from_numpy(maze_cut).float(),\n",
    "        torch.from_numpy(heat_norm).float()\n",
    "    ], dim=0)\n",
    "    \n",
    "    # Thêm chiều [1, 2, 9, 9]\n",
    "    return stack_map.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93e7d0",
   "metadata": {},
   "source": [
    "Hàm lấy vector chỉ hướng đích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c19ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_goal_vector(agent_position, goal_position, maze_size):\n",
    "\n",
    "    #agent_pos: (y, x)\n",
    "    #goal_pos: (y, x)\n",
    "    #maze_size: (H, W)\n",
    "\n",
    "    H, W = maze_size\n",
    "    \n",
    "    # 1. Tính khoảng cách thô\n",
    "    dy = goal_position[0] - agent_position[0]\n",
    "    dx = goal_position[1] - agent_position[1]\n",
    "    \n",
    "    # 2. Chuẩn hóa về khoảng [-1, 1]\n",
    "    dy_norm = dy / H\n",
    "    dx_norm = dx / W\n",
    "    \n",
    "    return torch.tensor([dy_norm, dx_norm], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2cc6c",
   "metadata": {},
   "source": [
    "Hàm cập nhật ô đã đi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c21b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_visit_count(heatmap, agent_position):\n",
    "    y, x = agent_position\n",
    "    heatmap[y, x] += 1\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923934c1",
   "metadata": {},
   "source": [
    "Tạo hàm ánh xạ số trong lớp quyết định và hướng đi của agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7baf0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_position(current_position, action):\n",
    "\n",
    "    y, x = current_position\n",
    "    \n",
    "    action_map = {\n",
    "        0: (-1, 0), # Lên\n",
    "        1: (1, 0),  # Xuống\n",
    "        2: (0, -1), # Trái\n",
    "        3: (0, 1)   # Phải\n",
    "    }\n",
    "    \n",
    "    dy, dx = action_map[action]\n",
    "    next_y = y + dy\n",
    "    next_x = x + dx\n",
    "    \n",
    "    return (next_y, next_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc96a2e",
   "metadata": {},
   "source": [
    "Tạo các lớp tích chập (convolution layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1db4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = nn.Sequential(\n",
    "    # Lớp 1: 9*9 -> 7*7\n",
    "    nn.Conv2d(in_channels=2, out_channels=16, kernel_size=3), \n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 2: 7*7 -> 5*5\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 3: 5*5 -> 3*3\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # Lớp 4: 3*3 -> 1*1\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "conv_layers = conv_layers.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55697fe",
   "metadata": {},
   "source": [
    "Tạo lớp quyết định"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e1f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_layer = nn.Linear(66, 4)\n",
    "decision_layer = decision_layer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a87bfd",
   "metadata": {},
   "source": [
    "Tạo hàm dự đoán bước tiếp theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaceff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_move(maze_np, heatmap_np, conv_layers, decision_layer, goal_position):\n",
    "    with torch.no_grad():\n",
    "        current_position = get_current_position(maze_np)\n",
    "\n",
    "        live_view = get_9x9_view(maze_np, heatmap_np, current_position)\n",
    "        view_4d = conv_layers(live_view)\n",
    "        view_flat = torch.flatten(view_4d, start_dim=1)  # [1, 64, 1, 1] -> [1, 64]\n",
    "    \n",
    "        goal_vec = get_goal_vector(current_position, goal_position, maze_np.shape).unsqueeze(0)  # [dy_norm, dx_norm] -> [[dy_norm, dx_norm]]\n",
    "    \n",
    "        final_input = torch.cat((view_flat, goal_vec), dim=1)\n",
    "    \n",
    "        action_logits = decision_layer(final_input)\n",
    "        action = torch.argmax(action_logits, dim=1).item()\n",
    "    \n",
    "        next_position = get_next_position(current_position, action)\n",
    "    \n",
    "    return action, next_position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d47e2d",
   "metadata": {},
   "source": [
    "Tạo hàm bước đi và hệ quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce13dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(action, maze_np, heatmap_np, goal_position):\n",
    "\n",
    "    curr_position = get_current_position(maze_np)\n",
    "    old_dist = abs(goal_position[0] - curr_position[0]) + abs(goal_position[1] - curr_position[1])\n",
    "    \n",
    "    next_position = get_next_position(curr_position, action)\n",
    "    nexty, nextx = next_position\n",
    "    \n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    if nexty < 0 or nexty >= maze_np.shape[0] or nextx < 0 or nextx >= maze_np.shape[1] or maze_np[nexty, nextx] == 1:\n",
    "        return -1, False \n",
    "        \n",
    "    if maze_np[nexty, nextx] == 9:\n",
    "        return 250, True\n",
    "    \n",
    "    if heatmap_np[nexty, nextx] > 0:\n",
    "        reward -= 0.02 * heatmap_np[nexty, nextx] \n",
    "    else:\n",
    "        reward -= 0.01 \n",
    "    \n",
    "    # Cập nhật trạng thái 2 lớp mê cung\n",
    "    maze_np[curr_position[0], curr_position[1]] = 0\n",
    "    maze_np[nexty, nextx] = 2\n",
    "    update_visit_count(heatmap_np, next_position)\n",
    "    \n",
    "    return reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc757ef",
   "metadata": {},
   "source": [
    "Chọn optimizer và cách tính loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484889e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parameters = list(conv_layers.parameters()) + list(decision_layer.parameters())\n",
    "optimizer = optim.Adam(all_parameters, lr=0.0001)\n",
    "\n",
    "losscal = nn.SmoothL1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c2198",
   "metadata": {},
   "source": [
    "Tạo mạng target độc lập"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "449b64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_conv = copy.deepcopy(conv_layers).to(device)\n",
    "target_decision = copy.deepcopy(decision_layer).to(device)\n",
    "\n",
    "for p in target_conv.parameters(): p.requires_grad = False\n",
    "for p in target_decision.parameters(): p.requires_grad = False\n",
    "\n",
    "def update_target_network():\n",
    "    target_conv.load_state_dict(conv_layers.state_dict())\n",
    "    target_decision.load_state_dict(decision_layer.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e18b25",
   "metadata": {},
   "source": [
    "Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f48ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_state(maze_np, heatmap_np, goal_position):\n",
    "\n",
    "    curr_position = get_current_position(maze_np)\n",
    "    view = get_9x9_view(maze_np, heatmap_np, curr_position)\n",
    "    \n",
    "    goal_vec = get_goal_vector(curr_position, goal_position, maze_np.shape).unsqueeze(0) # [2] -> [1, 2]\n",
    "    \n",
    "    return (view, goal_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bd3fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_buffer = []\n",
    "limit_memory = 100000\n",
    "\n",
    "def store_memory(state, action, reward, next_state, done):\n",
    "    if len(memory_buffer) >= limit_memory:\n",
    "        memory_buffer.pop(0)\n",
    "    memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "'''\n",
    "batch (List)\n",
    " ├── b_0 (Tuple)\n",
    " │    ├── [0] State: ([view_0], [goal_0])\n",
    " │    ├── [1] Action: 3\n",
    " │    ├── [2] Reward: -0.01\n",
    " │    ├── [3] Next State: ([n_view_0], [n_goal_0])\n",
    " │    └── [4] Done: False\n",
    " ├── b_1 (Tuple)\n",
    " │    ├── [0] State: ([view_1], [goal_1])\n",
    " │    ├── ...\n",
    " ├── ...\n",
    " └── b_31 (Tuple)\n",
    "'''\n",
    "\n",
    "def train_from_memory(batch_size=32, gamma=0.99):\n",
    "    if len(memory_buffer) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    batch = random.sample(memory_buffer, batch_size)\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    # Gộp 32 mảng 9x9 thành 1 khối [32, 2, 9, 9]\n",
    "    views = torch.cat([b[0][0] for b in batch]).to(device)\n",
    "    goal_vecs = torch.cat([b[0][1] for b in batch]).to(device)\n",
    "    actions = torch.tensor([b[1] for b in batch], dtype=torch.long).to(device)\n",
    "    rewards = torch.tensor([b[2] for b in batch], dtype=torch.float32).to(device)\n",
    "    \n",
    "    next_views = torch.cat([b[3][0] for b in batch]).to(device)\n",
    "    next_goal_vecs = torch.cat([b[3][1] for b in batch]).to(device)\n",
    "    dones = torch.tensor([b[4] for b in batch], dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Q-VALUE HIỆN TẠI (Main network)\n",
    "    v_out = conv_layers(views)\n",
    "    v_flat = torch.flatten(v_out, 1)\n",
    "    combined = torch.cat((v_flat, goal_vecs), dim=1)\n",
    "    q_values_all = decision_layer(combined) # [32, 4]\n",
    "    \n",
    "    q_value = q_values_all.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Q-TARGET\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        nv_out = target_conv(next_views)\n",
    "        nv_flat = torch.flatten(nv_out, 1)\n",
    "        n_combined = torch.cat((nv_flat, next_goal_vecs), dim=1)\n",
    "        \n",
    "        # Double DQN\n",
    "        # Main net chọn hành động\n",
    "        v_next_out = conv_layers(next_views)\n",
    "        v_next_flat = torch.flatten(v_next_out, 1)\n",
    "        next_combined_main = torch.cat((v_next_flat, next_goal_vecs), dim=1)\n",
    "        next_actions_main = decision_layer(next_combined_main).argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        # Target net đánh giá hành động\n",
    "        q_next_target_all = target_decision(n_combined)\n",
    "        max_next_q = q_next_target_all.gather(1, next_actions_main).squeeze(1)\n",
    "        \n",
    "        # Công thức Bellman: R + gamma * Q_next * (1 - done)\n",
    "        q_target = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "    loss = losscal(q_value, q_target)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(all_parameters, max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e0488",
   "metadata": {},
   "source": [
    "Đóng gói các hàm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf5d5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_and_store(maze_np, heatmap_np, goal_position, random_rate):\n",
    "\n",
    "    state_before = capture_state(maze_np, heatmap_np, goal_position)\n",
    "    \n",
    "    # Cơ chế Epsilon-Greedy \n",
    "    if np.random.rand() < random_rate:\n",
    "        action = np.random.randint(0, 4) # Chọn ngẫu nhiên 1 trong 4 hướng\n",
    "    else:\n",
    "        action, _ = predict_move(maze_np, heatmap_np, conv_layers, decision_layer, goal_position)\n",
    "    \n",
    "    reward, done = move(action, maze_np, heatmap_np, goal_position)\n",
    "\n",
    "    state_after = capture_state(maze_np, heatmap_np, goal_position)\n",
    "    \n",
    "    #Lưu\n",
    "    store_memory(state_before, action, reward, state_after, done)\n",
    "    \n",
    "    return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "629c1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename=\"maze_ai_checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'conv_state': conv_layers.state_dict(),       \n",
    "        'decision_state': decision_layer.state_dict(), \n",
    "        'optimizer_state': optimizer.state_dict(),                                                         \n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Đã lưu tham số vào file {filename}\")\n",
    "\n",
    "def load_checkpoint(filename=\"maze_ai_size_21.pth\"):\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    if torch.cuda.is_available():\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load(filename, map_location=torch.device('cpu'))\n",
    "    \n",
    "    # Nạp trọng số vào các lớp\n",
    "    conv_layers.load_state_dict(checkpoint['conv_state'])\n",
    "    decision_layer.load_state_dict(checkpoint['decision_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    \n",
    "    \n",
    "    print(f\"Đã tải thành công! Tiếp tục ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12ef0108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tải thành công! Tiếp tục ...\n"
     ]
    }
   ],
   "source": [
    "load_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a83d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b084d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu tham số vào file maze_ai_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "#save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vòng lặp train\n",
    "\"\"\"\n",
    "\n",
    "# Start setting\n",
    "current_size = 23\n",
    "max_size = 35\n",
    "num_episodes = 15000\n",
    "max_steps = int(current_size * current_size * 4) # Max steps theo size\n",
    "random_rate = 0.4\n",
    "target_update_steps = 1000 # Thời gian đóng băng mạng Target \n",
    "total_steps = 0\n",
    "\n",
    "# Winrate tracker\n",
    "win_history = np.zeros(100) # Lưu 100 ván gần nhất\n",
    "win_idx = 0\n",
    "played_enough = False\n",
    "\n",
    "print(f\"Bắt đầu huấn luyện từ Size {current_size}x{current_size}...\")\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    \n",
    "    maze_np = generate_maze(current_size) \n",
    "    heatmap_np = np.zeros_like(maze_np, dtype=np.float32)\n",
    "    goal_position = get_goal_position(maze_np)\n",
    "    \n",
    "    done = False\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done and step_count < max_steps:\n",
    "        # Sử dụng epsilon-greedy để di chuyển\n",
    "        reward, done = move_and_store(maze_np, heatmap_np, goal_position, random_rate)\n",
    "        \n",
    "        # Train\n",
    "        loss = train_from_memory(batch_size=64)\n",
    "        \n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        \n",
    "        # Cập nhật mạng Target\n",
    "        if total_steps % target_update_steps == 0:\n",
    "            update_target_network()\n",
    "\n",
    "        step_count += 1\n",
    "    \n",
    "    # Cập nhật win tracker\n",
    "    win_history[win_idx] = 1 if done else 0\n",
    "    win_idx = (win_idx + 1) % 100 # Tự reset win_idx\n",
    "    if win_idx == 0: played_enough = True\n",
    "\n",
    "    # Log thông tin ván đấu\n",
    "    status = \"THÀNH CÔNG\" if done else \"THẤT BẠI\"\n",
    "    print(f\"Ván {ep+1:4d} | {status} | Size: {current_size}x{current_size} | Bước: {step_count} | Điểm: {total_reward:.2f} | Loss: {loss:.4f} | Epsilon: {random_rate:.3f}\")\n",
    "\n",
    "    # Kiểm tra điều kiện upsize\n",
    "    if played_enough:\n",
    "        current_win_rate = np.mean(win_history) \n",
    "        \n",
    "        if current_win_rate >= 0.90 and current_size < max_size:\n",
    "            print(f\"Winrate 100 ván gần nhất: {current_win_rate*100:.1f}%\")\n",
    "            \n",
    "            # Lưu\n",
    "            save_checkpoint(filename=f\"maze_ai_size_{current_size}.pth\", epsilon=random_rate)\n",
    "            \n",
    "            # Upsize\n",
    "            current_size += 2\n",
    "            print(f\"Upsize: {current_size}x{current_size}\")\n",
    "            \n",
    "            # Reset setting\n",
    "            random_rate = 0.4\n",
    "            win_history.fill(0)\n",
    "            win_idx = 0\n",
    "            played_enough = False\n",
    "            memory_buffer.clear()\n",
    "            max_steps = int(current_size * current_size * 4) # Cập nhật lại giới hạn bước\n",
    "            print(f\"Đã reset Epsilon và Memory. Max steps mới: {max_steps}\\n\")\n",
    "\n",
    "    if played_enough and current_win_rate < 0.3 and random_rate < 0.2:\n",
    "        print(\"Increase epsilon due to low winrate\")\n",
    "        random_rate = 0.3\n",
    "\n",
    "    # Epsilon decay rate\n",
    "    if random_rate > 0.05:\n",
    "        random_rate *= 0.998 \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b632c117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dừng biểu diễn.\n",
      "THẤT BẠI! AI bị kẹt hoặc quá thời gian.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_ai_showcase_split(complexity=18, sleep_time=0.01):\n",
    "    # 1. Chế độ đánh giá\n",
    "    conv_layers.eval()\n",
    "    decision_layer.eval()\n",
    "    \n",
    "    # 2. Khởi tạo mê cung\n",
    "    maze_np = generate_maze(complexity)\n",
    "    heatmap_np = np.zeros_like(maze_np, dtype=np.float32)\n",
    "    goal_position = get_goal_position(maze_np)\n",
    "    current_pos = get_current_position(maze_np)\n",
    "    \n",
    "    path_history = [current_pos]\n",
    "    steps = 0\n",
    "    max_demo_steps = 1500\n",
    "    \n",
    "    # Thiết lập 2 bảng vẽ (Subplots)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    try:\n",
    "        is_done = False\n",
    "        while not is_done and steps < max_demo_steps:\n",
    "            \n",
    "            action, _ = predict_move(maze_np, heatmap_np, conv_layers, decision_layer, goal_position)\n",
    "            \n",
    "            # Lấy tầm nhìn 9x9 để hiển thị sang bảng bên phải\n",
    "            # live_view_tensor có dạng [1, 2, 9, 9]\n",
    "            live_view_tensor = get_9x9_view(maze_np, heatmap_np, current_pos)\n",
    "            # Chuyển về numpy để vẽ (Lấy channel 0 là maze structure)\n",
    "            ai_vision_img = live_view_tensor.squeeze(0)[0].cpu().numpy() \n",
    "            \n",
    "            # Thực hiện bước đi\n",
    "            reward, is_done = move(action, maze_np, heatmap_np, goal_position)\n",
    "            \n",
    "            # Cập nhật vị trí\n",
    "            if is_done:\n",
    "                current_pos = goal_position \n",
    "            else:\n",
    "                current_pos = get_current_position(maze_np)\n",
    "            \n",
    "            path_history.append(current_pos)\n",
    "            \n",
    "            # TOÀN CẢNH \n",
    "            ax1.clear()\n",
    "            ax1.imshow(maze_np, cmap='nipy_spectral')\n",
    "            if len(path_history) > 1:\n",
    "                hy, hx = zip(*path_history)\n",
    "                ax1.plot(hx, hy, color='yellow', linewidth=2, alpha=0.5)\n",
    "            ax1.set_title(f\"Tầm Nhìn Toàn Cảnh (Bước: {steps})\")\n",
    "            \n",
    "            # AI VISION (9x9) ---\n",
    "            ax2.clear()\n",
    "            # Hiển thị vùng 9x9 mà CNN đang xử lý\n",
    "            ax2.imshow(ai_vision_img, cmap='binary_r') \n",
    "            # Vẽ một chấm đỏ ở giữa bảng 2 đại diện cho chính AI\n",
    "            ax2.plot(4, 4, 'ro', markersize=15) \n",
    "            ax2.set_title(f\"Tầm nhìn cục bộ\")\n",
    "            \n",
    "            # Thêm các vạch lưới cho bảng 2 để nhìn rõ từng ô\n",
    "            ax2.set_xticks(np.arange(-0.5, 9, 1), minor=True)\n",
    "            ax2.set_yticks(np.arange(-0.5, 9, 1), minor=True)\n",
    "            ax2.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            display(fig)\n",
    "            time.sleep(sleep_time)\n",
    "            steps += 1\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Dừng biểu diễn.\")\n",
    "    finally:\n",
    "        plt.close(fig)\n",
    "        \n",
    "    if is_done:\n",
    "        print(f\"THÀNH CÔNG! AI đã về đích sau {steps} bước.\")\n",
    "    else:\n",
    "        print(f\"THẤT BẠI! AI bị kẹt hoặc quá thời gian.\")\n",
    "\n",
    "# Chạy thử\n",
    "run_ai_showcase_split(complexity=20, sleep_time=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec36cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.19.1</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m19.1\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"media\\jupyter\\RealAIMazeScene@2026-01-09@01-23-50.mp4\" controls autoplay loop style=\"max-width: 60%;\"  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%manim -v WARNING -qh RealAIMazeScene\n",
    "\n",
    "from manim import *\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class RealAIMazeScene(Scene):\n",
    "    def construct(self):\n",
    "        # --- 1. CHUẨN BỊ DỮ LIỆU ---\n",
    "        try:\n",
    "            raw_maze = generate_maze(20) \n",
    "            rows, cols = raw_maze.shape \n",
    "            goal_pos = get_goal_position(raw_maze)\n",
    "            start_pos = get_current_position(raw_maze)\n",
    "            conv_layers.eval()\n",
    "            decision_layer.eval()\n",
    "        except Exception as e:\n",
    "            print(f\"LỖI: {e}\")\n",
    "            return\n",
    "\n",
    "        path_data, vision_data = [], []\n",
    "        curr_p = start_pos\n",
    "        temp_maze, temp_heat = raw_maze.copy(), np.zeros_like(raw_maze, dtype=np.float32)\n",
    "        \n",
    "        # THU THẬP DỮ LIỆU (Sửa lỗi khớp Index)\n",
    "        for _ in range(1500): \n",
    "            # 1. Lưu trạng thái HIỆN TẠI của Agent vào cả 2 mảng\n",
    "            path_data.append(curr_p)\n",
    "            v_tensor = get_9x9_view(temp_maze, temp_heat, curr_p) #\n",
    "            vision_data.append(v_tensor.squeeze(0).cpu().numpy()) \n",
    "            \n",
    "            # 2. Dự đoán và di chuyển\n",
    "            action, _ = predict_move(temp_maze, temp_heat, conv_layers, decision_layer, goal_pos)\n",
    "            _, done = move(action, temp_maze, temp_heat, goal_pos)\n",
    "            \n",
    "            if done:\n",
    "                # 3. Khi xong, phải lưu thêm trạng thái tại đích để khớp index\n",
    "                path_data.append(goal_pos)\n",
    "                v_goal = get_9x9_view(temp_maze, temp_heat, goal_pos)\n",
    "                vision_data.append(v_goal.squeeze(0).cpu().numpy())\n",
    "                break\n",
    "            curr_p = get_current_position(temp_maze)\n",
    "\n",
    "        # --- 2. KHỞI TẠO GIAO DIỆN (GIỮ NGUYÊN) ---\n",
    "        dynamic_cell_size = min(5.0 / cols, 6.0 / rows)\n",
    "        WALL_COLOR = TEAL \n",
    "        PATH_COLOR = BLACK \n",
    "        START_COLOR = RED_E\n",
    "\n",
    "        global_grid = VGroup(*[\n",
    "            Square(side_length=dynamic_cell_size).set_stroke(width=0) \n",
    "            for _ in range(rows * cols)\n",
    "        ]).arrange_in_grid(rows=rows, cols=cols, buff=0)\n",
    "\n",
    "        for i, val in enumerate(raw_maze.flatten()):\n",
    "            if val == 1: global_grid[i].set_fill(WALL_COLOR, opacity=1)\n",
    "            elif val == 9: global_grid[i].set_fill(GREEN, opacity=1)\n",
    "            elif val == 2: global_grid[i].set_fill(START_COLOR, opacity=1)\n",
    "            else: global_grid[i].set_fill(PATH_COLOR, opacity=1)\n",
    "\n",
    "        maze_border = SurroundingRectangle(global_grid, color=WHITE, stroke_width=2, buff=0)\n",
    "        maze_view = VGroup(global_grid, maze_border)\n",
    "\n",
    "        vision_grid = VGroup(*[\n",
    "            Square(side_length=0.25).set_stroke(WHITE, 0.5).set_fill(PATH_COLOR, opacity=1)\n",
    "            for _ in range(81)\n",
    "        ]).arrange_in_grid(rows=9, cols=9, buff=0.02)\n",
    "        \n",
    "        vision_ui = VGroup(\n",
    "            Text(\"AI INTERNAL VISION\", font_size=20, color=WHITE).next_to(vision_grid, UP, buff=0.2),\n",
    "            vision_grid,\n",
    "            Dot(color=RED, radius=0.1).move_to(vision_grid[40])\n",
    "        )\n",
    "\n",
    "        # CĂN GIỮA ĐỐI XỨNG\n",
    "        main_layout = VGroup(maze_view, vision_ui).arrange(RIGHT, buff=1.5).center()\n",
    "\n",
    "        agent = Dot(color=RED, radius=dynamic_cell_size * 0.45)\n",
    "        agent.move_to(global_grid[start_pos[0] * cols + start_pos[1]])\n",
    "        \n",
    "        path_trace = TracedPath(lambda: agent.get_center(), stroke_color=YELLOW, stroke_width=dynamic_cell_size*25, stroke_opacity=0.7)\n",
    "\n",
    "        self.add(main_layout, agent, path_trace)\n",
    "\n",
    "        # --- 3. ANIMATION ---\n",
    "        self.wait(1)\n",
    "\n",
    "        for i in range(1, len(path_data)):\n",
    "            # Lấy dữ liệu 2 lớp cho bản đồ cục bộ\n",
    "            v_layers = vision_data[i] # Đã an toàn, không còn lỗi Index\n",
    "            maze_v = v_layers[0].flatten() \n",
    "            heat_v = v_layers[1].flatten() \n",
    "            \n",
    "            for j in range(81):\n",
    "                if maze_v[j] == 1:\n",
    "                    new_c = WALL_COLOR\n",
    "                else:\n",
    "                    # HIỆN NHIỆT CỤC BỘ:\n",
    "                    new_c = interpolate_color(PATH_COLOR, RED, 1.0 - heat_v[j])\n",
    "                \n",
    "                vision_grid[j].set(fill_color=new_c)\n",
    "\n",
    "            pos = path_data[i]\n",
    "            idx = int(pos[0] * cols + pos[1])\n",
    "            target_center = global_grid[idx].get_center()\n",
    "\n",
    "            self.play(\n",
    "                agent.animate.move_to(target_center),\n",
    "                run_time=0.08, \n",
    "                rate_func=linear\n",
    "            )\n",
    "\n",
    "        self.wait(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
